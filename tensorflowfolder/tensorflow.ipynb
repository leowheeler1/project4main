{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb0ba761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 00m 20s]\n",
      "val_accuracy: 0.8703879117965698\n",
      "\n",
      "Best val_accuracy So Far: 0.8798485994338989\n",
      "Total elapsed time: 00h 21m 45s\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies and set up TTL. This is being done here unlike the other modules because\n",
    "# the Tensorflow classifier needs 0 - numclasses - 1 instead of 1-5 so its slightly different.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Load and preprocess your CSV\n",
    "df = pd.read_csv(\"../datasets/allsymptomsdisease.csv\")\n",
    "df = df.dropna()\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(float)\n",
    "\n",
    "# Bin labels (and subtract 1 for 0-based indexing)\n",
    "def binner(scores):\n",
    "    output = []\n",
    "    for score in scores:\n",
    "        if score > 900:\n",
    "            output.append(1)\n",
    "        elif score > 650:\n",
    "            output.append(2)\n",
    "        elif score > 400:\n",
    "            output.append(3)\n",
    "        elif score > 200:\n",
    "            output.append(4)\n",
    "        else:\n",
    "            output.append(5)\n",
    "    return output\n",
    "\n",
    "df['label'] = binner(df['label'].tolist())\n",
    "df['label'] = df['label'] - 1\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "y = np.array(df['label'])\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define model builder for Keras Tuner\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('input_units', 128, 512, step=64),\n",
    "        activation=hp.Choice('input_activation', ['relu', 'tanh']),\n",
    "        input_shape=(X_train.shape[1],),\n",
    "        kernel_regularizer=regularizers.l2(hp.Float('l2_input', 1e-5, 1e-2, sampling='log'))\n",
    "    ))\n",
    "    if hp.Boolean('batchnorm_input'):\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_input', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Hidden Layers (1 to 5)\n",
    "    for i in range(hp.Int('num_layers', 2, 5)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f'units_{i}', 64, 512, step=64),\n",
    "            activation=hp.Choice(f'activation_{i}', ['relu', 'tanh']),\n",
    "            kernel_regularizer=regularizers.l2(hp.Float(f'l2_{i}', 1e-5, 1e-2, sampling='log'))\n",
    "        ))\n",
    "        if hp.Boolean(f'batchnorm_{i}'):\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', 0.1, 0.6, step=0.1)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compile with optimizer and learning rate\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    hp.Int('batch_size', 32, 128, step=32)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set up Keras Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='hp_tuning',\n",
    "    project_name='severity_classifier'\n",
    ")\n",
    "\n",
    "# Add early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Search for best model\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=40,\n",
    "             validation_split=0.2,\n",
    "             callbacks=[early_stop],\n",
    "             verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bc0bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "34/34 - 3s - 93ms/step - accuracy: 0.5493 - loss: 3.2047 - val_accuracy: 0.5345 - val_loss: 3.2749\n",
      "Epoch 2/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.7690 - loss: 2.4879 - val_accuracy: 0.6074 - val_loss: 3.1317\n",
      "Epoch 3/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.8142 - loss: 2.2829 - val_accuracy: 0.7162 - val_loss: 3.0002\n",
      "Epoch 4/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.8445 - loss: 2.1088 - val_accuracy: 0.7938 - val_loss: 2.8455\n",
      "Epoch 5/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.8693 - loss: 1.9945 - val_accuracy: 0.8146 - val_loss: 2.6668\n",
      "Epoch 6/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.8911 - loss: 1.8934 - val_accuracy: 0.8288 - val_loss: 2.4780\n",
      "Epoch 7/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9013 - loss: 1.8114 - val_accuracy: 0.8259 - val_loss: 2.2983\n",
      "Epoch 8/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9280 - loss: 1.7074 - val_accuracy: 0.8382 - val_loss: 2.1220\n",
      "Epoch 9/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9359 - loss: 1.6397 - val_accuracy: 0.8382 - val_loss: 1.9803\n",
      "Epoch 10/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9524 - loss: 1.5673 - val_accuracy: 0.8477 - val_loss: 1.8716\n",
      "Epoch 11/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9624 - loss: 1.5020 - val_accuracy: 0.8486 - val_loss: 1.7855\n",
      "Epoch 12/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9697 - loss: 1.4478 - val_accuracy: 0.8496 - val_loss: 1.7251\n",
      "Epoch 13/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9695 - loss: 1.4008 - val_accuracy: 0.8571 - val_loss: 1.6845\n",
      "Epoch 14/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.9770 - loss: 1.3484 - val_accuracy: 0.8581 - val_loss: 1.6500\n",
      "Epoch 15/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.9813 - loss: 1.2981 - val_accuracy: 0.8562 - val_loss: 1.6329\n",
      "Epoch 16/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.9830 - loss: 1.2639 - val_accuracy: 0.8666 - val_loss: 1.6049\n",
      "Epoch 17/40\n",
      "34/34 - 1s - 15ms/step - accuracy: 0.9886 - loss: 1.2081 - val_accuracy: 0.8657 - val_loss: 1.6011\n",
      "Epoch 18/40\n",
      "34/34 - 0s - 13ms/step - accuracy: 0.9912 - loss: 1.1728 - val_accuracy: 0.8657 - val_loss: 1.5906\n",
      "Epoch 19/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9889 - loss: 1.1364 - val_accuracy: 0.8647 - val_loss: 1.5841\n",
      "Epoch 20/40\n",
      "34/34 - 0s - 12ms/step - accuracy: 0.9884 - loss: 1.1050 - val_accuracy: 0.8666 - val_loss: 1.6005\n",
      "Epoch 21/40\n",
      "34/34 - 0s - 11ms/step - accuracy: 0.9893 - loss: 1.0671 - val_accuracy: 0.8704 - val_loss: 1.5954\n",
      "Epoch 22/40\n",
      "34/34 - 0s - 11ms/step - accuracy: 0.9924 - loss: 1.0253 - val_accuracy: 0.8675 - val_loss: 1.5878\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 1.6293\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Test Accuracy: 0.8529244661331177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.777     0.746     0.761       201\n",
      "           1      0.735     0.798     0.765       247\n",
      "           2      0.904     0.925     0.914       611\n",
      "           3      0.836     0.812     0.824       320\n",
      "           4      0.907     0.864     0.885       382\n",
      "\n",
      "    accuracy                          0.853      1761\n",
      "   macro avg      0.832     0.829     0.830      1761\n",
      "weighted avg      0.854     0.853     0.853      1761\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "\n",
    "# 2. Rebuild the model using those hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# 3. Refit model with optimal batch size\n",
    "best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=40,\n",
    "    batch_size=best_hp.get('batch_size'),\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 4. Evaluate on the test set\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test)\n",
    "y_pred = np.argmax(best_model.predict(X_test), axis=1)\n",
    "\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8335f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_units': 192, 'input_activation': 'tanh', 'l2_input': 0.00021777965369913825, 'batchnorm_input': False, 'dropout_input': 0.2, 'num_layers': 3, 'units_0': 256, 'activation_0': 'tanh', 'l2_0': 0.00427775755932623, 'batchnorm_0': True, 'dropout_0': 0.2, 'units_1': 64, 'activation_1': 'tanh', 'l2_1': 0.0005463063812883169, 'batchnorm_1': True, 'dropout_1': 0.1, 'optimizer': 'adam', 'learning_rate': 0.001677109135321793, 'batch_size': 96, 'units_2': 384, 'activation_2': 'tanh', 'l2_2': 0.0008854322452566148, 'batchnorm_2': False, 'dropout_2': 0.2, 'units_3': 256, 'activation_3': 'relu', 'l2_3': 0.0002851509951970098, 'batchnorm_3': False, 'dropout_3': 0.4, 'units_4': 128, 'activation_4': 'relu', 'l2_4': 0.00011908740025576155, 'batchnorm_4': True, 'dropout_4': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(best_hp.values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
